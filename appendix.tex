The appendix contains discussion of additional, more tangential topics.

\hypertarget{related-work}{\subsection{Related
work}\label{related-work}}

\hypertarget{gary-drescher-on-superrationality}{\subsubsection{Gary
Drescher on superrationality}\label{gary-drescher-on-superrationality}}

Superrationality, i.e. cooperation based on correlation, is a well-known
idea in decision theory
\parencite{Kuhn2017-tl,Horgan1981-hb,Hofstadter1983-az,Campbell1985-sx,Ahmed2014-ec}.
However, most authors do not discuss much beyond the basic idea. Chapter
7.2 of
\href{https://www.gwern.net/docs/2006-drescher-goodandreal.pdf}{\emph{Gary
Drescher's (2006)}} \emph{Good and Real} is the most extensive analysis
of the concept of which I am aware. Among other things, Drescher notes
that superrationality -- or, as he calls it, subjunctive reciprocity --
can be applied broadly as a justification for ``altruistic'' behavior,
which I discuss in section
\ref{superrationality-and-morality}. He also points out that superrationality removes the
need for reciprocity (see section
\ref{no-reciprocity-needed-whom-to-treat-beneficially}).

Although Drescher discusses the Everett interpretation of quantum
physics in his book, he does not connect it with superrationality. His
considerations thus focus on superrationality among agents on Earth,
which I would argue to be quite weak (see section
\ref{superrational-cooperation-on-earth}). Nonetheless, his account of superrationality
is more thorough than any other I have seen, and strongly influenced the
\ref{superrationality}
chapter of this paper.

\hypertarget{acausal-trade}{\subsubsection{Acausal
trade}\label{acausal-trade}}

\href{https://wiki.lesswrong.com/wiki/Acausal_trade}{\emph{Acausal
trade}} is another (mostly informally discussed) form of cooperation
based on non-causal decision theories and has often been combined with
the multiverse concept. However, the mechanism usually discussed under
the term acausal trade differs from superrationality. Instead of
assuming the similarity between two agents, acausal trade merely
requires them them to have models of each other. For example, the two
agents may know each other's source code.\footnote{Alternatively, one of
  the two agents can observe the others' behavior. In this case, only
  the other agent needs a model.} The main technical difficulty here is
to avoid the infinite loop associated with this mutual modeling. The
basic idea is that both agents adopt the policy of cooperating if and
only if the other agent cooperates. This is intended to incentivize
cooperation in a way reminiscent of causal cooperation via tit for tat.
One can also view this policy of mirroring the other agent's strategy as
a way to create correlations between the decisions of similar agents.
However, if both agents use this policy, they run into an infinite loop:
To make a decision, the first agent has to find out (probabilistically)
what the second agent does. But to do so, it has to find out what the
first agent does, which in turn means finding out what the second agent
does, etc. As illustrated by
\href{https://arxiv.org/abs/1401.5577}{\emph{Barasz et al. (2014)}},
this problem can sometimes be solved, thus making it rational for two
programs with knowledge of one another's source code to cooperate with
each other \parencite{LaVictoire2014-sv,Critch2016-cx}.

Superrationality may be seen as a special case of acausal trade in which
the agents' knowledge implies the correlation directly, thus avoiding
the need for explicit mutual modeling and the complications associated
with it. This makes superrationality much more easy to apply than
acausal trade. Consequently, whereas I propose that humans should reason
superrationally, acausal trade is usually discussed only in the context
of superintelligent AIs (e.g.,
\href{http://www.nickbostrom.com/papers/porosity.pdf}{\emph{Bostrom
2014}}).

\subsubsection{Various mentions of multiverse-wide
superrationality}\label{various-mentions-of-multiverse-wide-superrationality}

While I am not aware of any substantive discussion of MSR, some have
mentioned it as a side remark, or proposed specific applications:

\begin{itemize}
\item
  \parencite{Bostrom2014-gy} writes: ``We might {[}..{]}
  hope that some of the other civilizations building AIs would {[}also
  implement their AI in a way that enables trade (see sections
  \ref{making-an-ai-come-up-with-superrational-cooperation-on-its-own} and
  \ref{compromise-friendly-backup-utility-functions}){]}, and perhaps the probability that
  they would do so would be increased if we decided to take such a
  cooperative path.'' On page 14, he also argues that one should perhaps
  diversify the values of an AI for a similar reason.
\item
  Almond
  (\href{https://casparoesterheld.files.wordpress.com/2017/03/correlation2.pdf}{\emph{2010}},
  ch. 4) discusses a few examples of how we can utilize the correlation
  with other civilizations. One of them is discussed in section
  \ref{simulations}.
\end{itemize}

\hypertarget{many-agents}{\subsection{Many agents}\label{many-agents}}

One essential ingredient of multiverse-wide superrationality is the
number of intelligent agents that exist. We have, to some people's
\href{https://en.wikipedia.org/wiki/Fermi_paradox}{\emph{surprise}}, not
(yet) found extraterrestrial life in the observable universe. However,
the universe, or multiverse, probably extends far beyond the region we
can observe. More likely than not, it contains so many agents that the
number of humans on Earth pales in comparison.

Unfortunately, physics and cosmology are not the most accessible of
fields. Introductions tend to either involve advanced mathematical
notation or
\href{http://lesswrong.com/lw/ip/fake_explanations/}{\emph{fuzzy
explanations}} with terms like ``space-time distortions'', ``waves'',
space being referred to as ``flat'', dimensions as ``curled up'', etc.
that seem hard to understand without looking at their technical meaning.
For an overview of the latter kind, consider Tegmark's
\parencite{Tegmark2003-sl} \emph{Parallel Universes},
which also discusses the number of intelligent agents specifically.
Another, even broader popular science
\href{https://en.wikipedia.org/wiki/Multiverse\#Brian_Greene.27s_nine_types}{\emph{overview}}
is given by \parencite{Greene2011-hv}. In this section, we
focus on the easiest to understand aspects. As mentioned in chapter
\ref{introduction-the-basic-idea}, we will use the term ``multiverse'' to also refer
to, say, a spatially infinite universe.

It is important to note that most talk about multiverses is not
something physicists make up out of thin air as an intellectual
exercise. Instead, certain well-tested theories in physics and cosmology
\emph{imply} the existence of a large universe or multiverse. One of the
easier to understand examples is the
\href{https://en.wikipedia.org/wiki/Many-worlds_interpretation}{\emph{Everett
or many-worlds interpretation}} (MWI) of
\href{https://en.wikipedia.org/wiki/Quantum_mechanics}{\emph{quantum
mechanics}}. For an introduction, consider Eliezer
\parencite{Yudkowsky2015-tz}
\href{https://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence}{\emph{introduction}},
which makes a strong case for MWI and goes through some of the issues
typically discussed, like
\href{http://lesswrong.com/lw/q4/decoherence_is_falsifiable_and_testable/}{\emph{falsifiability/testability}}
and the
\href{http://lesswrong.com/lw/pb/belief_in_the_implied_invisible/}{\emph{law
of parsimony}}
\parencite{Tegmark2001-lm,Tegmark2007-mx,Vaidman2016-cv}.
For a more critical account, see, e.g.,
\parencite{Kent1997-lm}. Tentative polls of physicists'
opinions on MWI
\href{https://en.wikipedia.org/wiki/Many-worlds_interpretation\#Polls}{\emph{indicate}}
that between 10\% and 50\% agree with MWI
\parencite{Tipler1994-di,Tegmark1997-dd,Nielsen2004-lu,Emerson2006-go}.
But the many-worlds interpretation of quantum physics is not the only
case that can be made for a universe with a very large or infinite
number of agents. In fact other arguments are probably more widely
accepted.

Maybe the least ``extraordinary'' hypothesis implying the existence of
many agents is one which says that this universe is
\href{https://en.wikipedia.org/wiki/Shape_of_the_universe}{\emph{spatially
infinite}}. According to Tegmark
\parencite{Tegmark2003-sl}, ``this spatially infinite
cosmological model is in fact the simplest and most popular one on the
market today''.

Even if the universe is spatially finite and small, it may still contain
a lot of civilizations that cannot interact with each other if it is
temporally infinite. For example, on a
\href{https://en.wikipedia.org/wiki/Cyclic_model}{\emph{cyclic model}}
the universe goes through an indefinite number of oscillations of
expansion and collapse. If sufficiently many of these oscillations give
rise to different civilizations, then these civilizations can cooperate
with each other.

Another more complicated yet popular cosmological theory is
\href{https://en.wikipedia.org/wiki/Eternal_inflation}{\emph{eternal
inflation}} as described in ch. II of Tegmark's \emph{Parallel
Universes}. Eternal inflation postulates the existence of multiple
universes which not only differ in initial conditions but also in their
number of dimensions, their sets of
\href{https://en.wikipedia.org/wiki/Elementary_particle}{\emph{fundamental
particles}}, and their physical constants.

On the more speculative (but also more accessible) side, there are
various forms of
\href{https://en.wikipedia.org/wiki/Modal_realism}{\\emph{modal
realism}} (sometimes also called \emph{mathematical monism}), the view
that every ``possible world'' exists in the same way in which our world
exists. While modal realism is controversial and rarely discussed by
physicists,
\href{http://reducing-suffering.org/why-does-physics-exist/\#Maybe_theres_no_such_thing_as_existence}{\emph{some
view it as an elegant solution}} to some philosophical problems. Modal
realist theories are also very
\href{http://lesswrong.com/lw/jp/occams_razor/}{\emph{simple}}, although
to make predictions with them, they require supplementation with
\href{https://en.wikipedia.org/wiki/Indexicality\#Extensions}{\emph{indexical}}
information about which agent in which possible world we are
(\href{https://arxiv.org/abs/0912.5434}{\emph{Hutter 2010}}, ch. 3). For
different starting points for thinking about modal realism, see any of
the following:
\parencite{Lewis1986-rk,Tegmark1998-mf,Tegmark2008-sq,Tegmark2014-mw,Schmidhuber1997-vd}.

Acting under the assumption of modal realism is associated with some
complications, however. In particular, because literally everything can
happen, everything will happen in some possible world, no matter what we
do. Thus no action seems to be better than another. (TODOLaTeX
Oesterheld unpublished: No-free-lunch and acting in Tegmark Level IV.)

Besides the arguments in favor of assigning a high probability to living
in a universe with many agents, there also exists a prudential reason to
act as though one lives in a large universe. Even if we only assign, for
example, a 50\% probability to the existence of other civilizations, our
decisions matter much more if there are more other agents with whom we
are correlated. Thus, we should optimize our decisions more for the
large universe. This line of reasoning does not work for all value
systems, however. For example, in terms of multiverse-wide
\href{https://en.wikipedia.org/wiki/Average_and_total_utilitarianism}{\emph{average
welfare}}, our influence may be much bigger if the universe was very
small. An average utilitarian
\href{https://casparoesterheld.com/2017/03/15/the-average-utilitarians-solipsism-wager/}{\emph{may}}
thus follow the opposite prudential argument and act as though the
universe was small.

\subsection{Testability of
superrationality}\label{testability-of-superrationality}

Eliezer Yudkowsky
(\href{https://intelligence.org/files/TDT.pdf}{\emph{2010}}, ch. 13)
writes:

\begin{quote}
If a dispute boils down to a testable hypothesis about the consequences
of actions, surely resolving the dispute should be easy! We need only
test alternative actions, observe consequences, and see which
probability assignment best matches reality.

Unfortunately, evidential decision theory and causal decision theory are
eternally unfalsifiable---and so is {[}timeless decision theory
(TDT){]}. The dispute centers on the consequences of logically
impossible actions, counterfactual worlds where a deterministic
computation returns an output it does not actually return. In evidential
decision theory, causal decision theory, and TDT, the observed
consequences of the action actually performed will confirm the
prediction made for the performed action. The dispute is over the
consequences of decisions not made.
\end{quote}

This also means that superrationality itself -- not only its application
to agents in faraway parts of the multiverse -- is untestable.

\hypertarget{do-people-reason-superrationally}{\subsection{Do people
reason superrationally?}\label{do-people-reason-superrationally}}

Do people already apply superrational reasoning when interacting with
each other on Earth? Certainly,
\href{https://casparoesterheld.com/2017/06/27/a-survey-of-polls-on-newcombs-problem/}{\emph{many
disagree with CDT's choice in contrived examples like Newcomb's
problem}} or the prisoner's dilemma against a copy, but does it ever
influence their real-world decisions? When conducting a donation game
for his \emph{Scientific American} article,
\href{https://www.gwern.net/docs/1985-hofstadter\#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery}{\emph{Hofstadter
(1983)}} asked the participants to explain their reasoning:

\begin{quote}
I would like to quote to you some of the feelings expressed by my
friends caught in this deliciously tricky situation. {[}...{]} Martin
Gardner (yes, I asked Martin to participate) vividly expressed the
emotional turmoil he and many others went through. ``Horrible dilemma'',
he said. ``I really don't know what to do about it. If I wanted to
maximize my money, I would choose D and expect that others would also;
to maximize my satisfactions, I'd choose C, and hope other people would
do the same (by the Kantian imperative). I don't know, though, how one
should behave rationally. You get into endless regresses: `If they all
do X, then I should do Y, but then they'll anticipate that and do Z, and
so . . .' You get trapped in an endless whirlpool. It's like Newcomb's
paradox.'' So saying, Martin defected, with a sigh of regret.

In a way echoing Martin's feelings of confusion, Chris Morgan said,
``More by intuition than by anything else, I'm coming to the conclusion
that there's no way to deal with the paradoxes inherent in this
situation. So I've decided to flip a coin, because I can't anticipate
what the others are going to do. I think - but can't know - that they're
all going to negate each other.'' So, while on the phone, Chris flipped
a coin and ``chose'' to cooperate.

Sidney Nagel was very displeased with his conclusion. He expressed great
regret: ``I actually couldn't sleep last night because I was thinking
about it. I wanted to be a cooperator, but I couldn't find any way of
justifying it. The way I figured it, what I do isn't going to affect
what anybody else does. I might as well consider that everything else is
already fixed, in which case the best I can do for myself is to play a
D.''

{[}...{]}

`C' is the answer I was hoping to receive from everyone. I was not so
optimistic as to believe that literally everyone would arrive at this
conclusion, but I expected a majority would - thus my dismay when the
early returns strongly favored defecting. As more phone calls came in, I
did receive some C's, but for the wrong reasons.
\href{http://en.wikipedia.org/wiki/Dan\%20Dennett}{\emph{Dan Dennett}}
cooperated, saying, ``I would rather be the person who bought the
Brooklyn Bridge than the person who sold it. Similarly, I'd feel better
spending \$3 gained by cooperating than \$10 gained by defecting.''

\href{http://en.wikipedia.org/wiki/Charles\%20Brenner}{\emph{Charles
Brenner}}, who I'd figured to be a sure-fire D, took me by surprise and
C'd. When I asked him why, he candidly replied, ``Because I don't want
to go on record in an international journal as a defector.'' Very well.
Know, World, that Charles Brenner is a cooperator!

Many people flirted with the idea that everybody would think ``about the
same'', but did not take it seriously enough. Scott Buresh confided to
me: ``It was not an easy choice. I found myself in an oscillation mode:
back and forth. I made an assumption: that everybody went through the
same mental processes I went through. Now I personally found myself
wanting to cooperate roughly one third of the time. Based on that figure
and the assumption that I was typical, I figured about one third of the
people would cooperate. So I computed how much I stood to make in a
field where six or seven people cooperate. It came out that if I were a
D, I'd get about three times as much as if I were a C. So I'd have to
defect. Water seeks out its own level, and I sank to the lower
right-hand corner of the matrix.'' At this point, I told Scott that so
far, a substantial majority had defected. He reacted swiftly: ``Those
rats - how can they all defect? It makes me so mad! I'm really
disappointed in your friends, Doug.'' So was I, when the final results
were in: Fourteen people had defected and six had cooperated {[}...{]}.
\end{quote}

Based on this anecdotal evidence, people do not consider superrationally
in this real-world donation game, although they sometimes make the
superrational choice for other reasons. In general, there are many
hypotheses about why people sometimes cooperate that do not involve any
sort of acausal reasoning. Presumably, many are either unaware of the
causal line of reasoning or do not properly set up the proposed
experiment in their mind. For instance,
\parencite{Yudkowsky2015-tz} argues that people cannot
pretend to be selfish and therefore take the reward to the other player
into account.
\href{http://personal.lse.ac.uk/kanazawa/pdfs/JNPE2013.pdf}{\emph{Kanazawa
and Fontaine (2013)}} demonstrate that ``the subject's behavioral choice
(cooperation vs. defection) varied significantly as a function of
subconscious perception of cues to possible reputational effect (in the
form of a video image of another subject in the experiment).'' Cultural
norms
\href{https://en.wikipedia.org/wiki/Public_goods_game\#Applications_to_sociology}{\emph{are}}
also often invoked to explain cooperation.\footnote{Data from other
  games with similarly dissatisfying Nash equilibria can be used as
  further tests of such models of human reasoning. For example, Basu
  (\href{http://www.cs.virginia.edu/~robins/The_Travelers_Dilemma.pdf}{\emph{2007}})
  reviews research on people's choices in the traveler's dilemma. He
  also hypothesizes that many people do not go with the Nash equilibrium
  because of hardwired altruism.} This short list of example
explanations is by no means an exhaustive review of the literature on
why people cooperate in one-shot games like the prisoner's dilemma and
public goods games.

\parencite{Drescher2006-ky} defends the opposite view. He
argues that although people do not act according to some systematic
acausal decision theory, they nevertheless implicitly account for
acausal reasoning into account implicitly. Similarly,
\href{https://sl4librarian.files.wordpress.com/2016/12/two-bird-deaths-one-throw-leslie.pdf}{\emph{Leslie
(1991, ch. 7)}} writes, ``perhaps the germs of {[}evidentialist
reasoning{]} are already present in thoughts influential in getting
people into polling booths, thoughts on the lines of `What if everybody
in my party stayed in bed?'''. Perhaps this ``lack of a correct explicit
decision theory leaves the solution somewhat vulnerable to seemingly
sound counterarguments, and thus leaves the solution's influence
somewhat tentative'' (Drescher, p. 289). This could explain why many
people who have considered the problem in great detail do not go with
the recommendation of acausal arguments despite potentially having an
innate intuition for them.

Recently,
\href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.464\&rep=rep1\&type=pdf}{\emph{Fischer
(2009)}} has proposed that people do engage in superrationality-like
reasoning. In a study, he showed that participants' cooperation in a
one-shot prisoner's dilemma correlated with reported probabilities of
the opponent making the same choice as oneself (cf.
\href{https://sl4librarian.files.wordpress.com/2017/01/krueger2012-social-projection.pdf}{\emph{Krueger
et al. 2012)}}.

One further piece of evidence in favor of this hypothesis is that
cooperation decreases when people learn about the other person's choice
before they make their own choice.
\href{https://www.researchgate.net/publication/222823221_Understanding_cooperation_in_the_Prisoner\%27s_Dilemma_game}{\emph{Pothos
et al. (2010)}} write:

\begin{quote}
Shafir and Tversky
(\href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.371.8926\&rep=rep1\&type=pdf\#page=720}{\emph{1992}};
\href{http://bacon.umcs.lublin.pl/~lukasik/wp-content/uploads/2010/12/A-Quantum-Information-Processing-p131.pdf}{\emph{Busemeyer,
Matthew, \& Wang, 2006}};
\href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.318.6368\&rep=rep1\&type=pdf}{\emph{Croson,
1999}};
\href{https://www.researchgate.net/publication/285712430_Examining_whether_there_is_a_disjunction_effect_in_prisoner\%27s_dilemma_games}{\emph{Li
\& Taplin, 2002}};
\href{https://www.researchgate.net/publication/240286082_The_Disjunction_Effect_in_Choice_Under_Uncertainty}{\emph{Tversky
\& Shafir, 1992}}) created a well-known modification to the Prisoner's
Dilemma game: in some trials, participants were told what the other
player was doing. Unsurprisingly, when participants were told that the
other person decided to D, then their probability to D was 97\%; and
when they were told that the other person decided to C, then their
probability of D was 84\%. However, in trials (within participants
design) when participants were not told what the other person did, the
probability to D dropped to 63\%.
\end{quote}

While inconsistent with mere causal reasoning, this can be explained
with acausal reasoning. Given knowledge of the other person's decision,
the evidential impact of cooperation diminishes (cf. section
\ref{lack-of-knowledge-is-evidential-power-part-i-the-other-agents}).
Moreover, this behavior cannot be explained by reputational issues or
altruistic preferences, which would, if anything, suggest that one would
return the favor upon learning that the other person cooperated.
However, the standard explanation attributes this behavior to people's
irrationality.

Overall, I lean towards the view that people do not have strong acausal
intuitions in day-to-day scenarios, which means that people who
\emph{do} take such considerations seriously do not correlate strongly
with the average person.

\hypertarget{the-evolution-of-superrationality}{\subsection{The
evolution of superrationality}\label{the-evolution-of-superrationality}}

Even though superrationality is not testable in any given situation, it
does produce actual benefits. This much is clear even to a causal
decision theorist, who would thus self-modify to take some, though not
all, acausal considerations into account (see section
\ref{cdt-would-self-modify-to-behave-like-a-non-causal-decision-theory-in-some-newcomb-like-problems}). For the
same reasons, a causal decision theorist would also program an AI to
take these considerations into account.

Similarly, evolution favors agents that take some superrational
considerations into account. For example, imagine a planet on which near
copies of agents are created on a regular basis. They then interact with
each other in cooperation and coordination games like the donation game.
To facilitate evolution, copies are created in proportion to the payoffs
in the cooperative games. On this planet, superrational agents -- i.e.
those who cooperate with close copies and other correlated agents, while
defecting against uncorrelated agents -- have an evolutionary advantage
over CDT-based agents who always defect. They will, on average, receive
higher payoffs and thus reproduce more successfully. Evolution can,
therefore, in principle favor genes (and
``\href{https://en.wikipedia.org/wiki/Meme}{\emph{memes}}) that
promote superrational reasoning.

In some sense, the described planet resembles ours. On Earth, ``near
copies'' of humans are created via reproduction and upbringing.
Moreover,
\href{http://mindingourway.com/newcomblike-problems-are-the-norm/}{\emph{many}}
\href{https://en.wikipedia.org/wiki/Prisoner\%27s_dilemma\#Real-life_examples}{\emph{have}}
pointed out that scenarios paralleling the prisoner's dilemma and public
goods games were common in our ancestral environment.

In principle, such considerations also apply to multiverse-wide
application of superrationality. That is, multiverse-wide evolution
favors creatures who increase the genetic fitness of agents with similar
decision algorithms elsewhere in the multiverse. In practice, however, I
suspect that almost all creatures with at most human capabilities are
unable to benefit any genomes other than those extant in their
environments.

\hypertarget{superrational-cooperation-on-earth}{\subsection{Superrational
cooperation on Earth}\label{superrational-cooperation-on-earth}}

Some, e.g.
\href{http://www.jstor.org/stable/2254984?seq=1\#page_scan_tab_contents}{\emph{Leslie
(1991}}, ch. 8) and
\href{http://mindingourway.com/newcomblike-problems-are-the-norm/}{\emph{Nate
Soares}}, have argued that superrationality and acausal decision theory
are relevant even in daily interactions between humans on Earth without
considering the multiverse. Drescher
\parencite{Drescher2006-ky} even contends that it is an
argument for egoists to behave altruistically. Others, like Almond
(\href{https://casparoesterheld.files.wordpress.com/2016/12/almond_edt_1.pdf}{\emph{2010a}},
ch. 4.6;
\href{https://web.archive.org/web/20120310010225/http://www.paul-almond.com/Correlation2.pdf}{\emph{2010b}},
ch. 1) or \parencite{Ahmed2014-ec}, maintain the opposite
position, i.e. that acausal reasoning is rarely relevant. I will argue
for the latter claim. Indeed, my belief that acausal cooperation is
usually inapplicable is the reason why this paper discusses its
application to the multiverse rather than more ``down to Earth''
scenarios.

\subsubsection{Fewer agents}\label{fewer-agents}

Superrationality becomes relevant in the multiverse because it contains
so many disconnected agents. Thus, even if the correlation with every
individual agent's decision is small, the overall acausal impact of our
decisions dominates (see section
\ref{the-relative-importance-of-superrational-cooperation-an-example-calculation}). The smaller the number of agents, the higher the
relative importance of the \emph{causal} implications of our actions.
Since the number of agents on Earth is comparably small, causal
considerations may well dominate.

\subsubsection{Argument from evolution: Superrationality did not evolve
(strongly)}\label{argument-from-evolution-superrationality-did-not-evolve-strongly}

We argued that superrational compromise can, under certain conditions,
evolve by natural means and that many of the respective conditions are
even met on Earth (see section
\ref{the-evolution-of-superrationality}). Hence, the mere observation that
most people do not reason superrationally (see section
\ref{do-people-reason-superrationally}) makes a case against its importance.

\subsubsection{Causal cooperation seems more
important}\label{causal-cooperation-seems-more-important}

Beyond the argument from evolution and empirical evidence, there are
also principled reasons against the relevance of superrationality on
Earth. One is that humans rarely face one-shot prisoner's dilemmas
against strongly correlated agents. Instead, their interactions are
usually iterated and open to mutual causal influence. As a result,
causal cooperation mechanisms apply, at least in principle (see section
\ref{no-reciprocity-needed-whom-to-treat-beneficially} for references to introductions on causal cooperation).
Surveying the vast literature on causal cooperation and how it compares
to superrational cooperation is beyond the scope of this paper, but two
key points are worth highlighting. First, rational agents establish
causal cooperation in a surprisingly wide range of situations. Second,
successful strategies like
\href{https://en.wikipedia.org/wiki/Tit_for_tat}{\emph{tit-for-tat}} or
Gradual \parencite{Beaufils1997-mi} tend to start the game
by cooperating and never defect unless the other side starts defecting.
Together, this suggests that sufficiently smart people -- which, I
assume, includes most agents who might apply superrationality -- are
capable of strong cooperation with one another without ever having to
invoke superrationality.

\subsubsection{Hard-wired alternatives}\label{hard-wired-alternatives}

Superrationality is not the only solution to the adaptive challenge of
having to cooperate with similar agents (e.g., members of the same tribe
and relatives). One alternative is to hard-wire creatures to cooperate
with very similar agents and defect against everyone else. This approach
to ensuring cooperation has received some attention in the literature,
although it is not nearly as widely known as the mechanisms of causal
cooperation (see, e.g.
\href{http://www.mcafee.cc/Papers/PDF/EffectiveComputability.pdf}{\emph{McAfee
(1984)}},
\href{https://sl4librarian.files.wordpress.com/2016/12/howard1988.pdf}{\emph{Howard
(1988)}} or
\href{https://ie.technion.ac.il/~moshet/progeqnote4.pdf}{\emph{Tennenholtz
(n.d.)}}).

\hypertarget{superrationality-and-morality}{\subsection{Superrationality
and morality}\label{superrationality-and-morality}}

Cooperation is often invoked as an argument why altruistic behavior and
following moral rules is rational
\parencite{Dawkins1976-cd,Greene2013-sq}. In many ways, the
application of \emph{superrational} cooperation resembles altruistic
behavior even more closely. For example, superrationality implies that
we should help a value system even if we know for certain that no agent
with this value system will or can reciprocate (see section
\ref{no-reciprocity-needed-whom-to-treat-beneficially}). Additionally, in
suggesting that we treat others the way they would like to be treated
(in order to make it more likely that others treat us the way \emph{we}
would like to be treated), superrationality
\href{http://briantomasik.com/interpreting-the-categorical-imperative/\#Categorical_imperative_as_decision_theory}{\emph{resembles}}
Kant's
\href{https://en.wikipedia.org/wiki/Categorical_imperative}{\emph{categorical
imperative}} and the
\href{https://en.wikipedia.org/wiki/Golden_Rule}{\emph{Golden Rule}}.
Once someone is updateless, she has additional reasons to be nice to
others: even if she learns that they do not or will not cooperate, she
would potentially still behave nicely toward them (see section
\ref{lack-of-knowledge-is-evidential-power-part-ii-taking-a-step-back}).
Similarly, if she were ever to find herself in a situation resembling
the Remote-controlled cake maker thought experiment (see section
\ref{updateless-weights}),
where she knows that cooperation hurts her values, she might still
sacrifice her values. Some implications of superrationality thus bear a
close resemblance to altruistic or moral behavior.

In ch. 7.2.1 of
\href{https://www.gwern.net/docs/2006-drescher-goodandreal.pdf}{\emph{Good
and Real}}, Gary Drescher makes similar points regarding the similarity
between superrational cooperation and real altruism. However, he goes
further by arguing that superrational cooperation is the \emph{basis}
for morality -- a way of ``deriving
\href{https://en.wikipedia.org/wiki/Is\%E2\%80\%93ought_problem}{\emph{ought
from i}}. I will discuss two questions that might arise
from this argument: is altruistic action derived from self-interest
really the essence of morality or altruism? And: is superrationality
sufficient for arriving at the desired altruistic conclusions?

\hypertarget{real-altruism}{\subsubsection{Real
altruism}\label{real-altruism}}

\parencite{Yudkowsky2015-tz} writes:

\begin{quote}
Consider the following, and ask which of these two philosophers is
really the altruist, and which is really selfish?

``You should be selfish, because when people set out to improve society,
they meddle in their neighbors' affairs and pass laws and seize control
and make everyone unhappy. Take whichever job that pays the most money:
the reason the job pays more is that the
\href{https://en.wikipedia.org/wiki/Efficient-market_hypothesis}{\emph{efficient
market}} thinks it produces more value than its alternatives. Take a job
that pays less, and you're second-guessing what the market thinks will
benefit society most.''

``You should be altruistic, because the world is an iterated Prisoner's
Dilemma, and the strategy that fares best is
\href{https://en.wikipedia.org/wiki/Tit_for_tat}{\emph{Tit for Tat}}
with initial cooperation. People don't \emph{like} jerks.
\href{https://en.wikipedia.org/wiki/Nice_Guys_Finish_First}{\emph{Nice
guys really do finish first}}. Studies show that people who contribute
to society and have a sense of meaning in their lives, are happier than
people who don't; being selfish will only make you unhappy in the long
run.''

Blank out the \emph{recommendations} of these two philosophers, and you
can see that the first philosopher is using strictly prosocial criteria
to \emph{justify} his recommendations; to him, what validates an
argument for selfishness is showing that selfishness benefits everyone.
The second philosopher appeals to strictly individual and hedonic
criteria; to him, what \emph{validates} an argument for altruism is
showing that altruism benefits him as an individual: higher social
status or more intense feelings of pleasure.

So which of these two is the \emph{actual} altruist?
\end{quote}

Yudkowsky elaborates in the rest of the chapter.

The point he is making is that ``actual altruism'' is usually understood
to mean caring about others, rather than merely behaving altruistically
based on egoistic reasoning. Verbal disputes about the meaning of ``true
altruism'' aside, there is a difference between having the welfare of
others as part of one's goal on the one hand, and benefitting others for
egoistic (or other non-altruistic or amoral) reasons on the other. I am
an altruist of the former kind, but cooperation (whether superrational
or not) only supports altruism of the latter kind. I would think that
most other people are also altruists of the former kind (in addition to
sometimes being altruists of the latter kind).\footnote{Note that while
  humans evolved to spread their genes as much as possible, they are
  neither pure fitness maximizers nor pure egoists (in the sense of not
  caring about others' welfare). Our altruistic intentions evolved for
  reasons of
  \href{https://en.wikipedia.org/wiki/Fitness_\%28biology\%29}{\emph{fitness}},
  but that does not mean they are not genuine altruistic intentions
  \parencite{Yudkowsky2015-tz,Cosmides1995-bz,Wright1995-po}.}

Altruism of the latter kind also does not
``\href{https://en.wikipedia.org/wiki/Is\%E2\%80\%93ought_problem}{\emph{derive
\emph{ought}} \emph{from \emph{is}}}\footnote{This is no surprise, as
  deriving ought from is cannot -- at least in my view -- be done.}, as
Drescher promises in chapter 7 of \emph{Good and Real}. Instead, it
derives (potentially unexpected) action recommendations from an already
existing ought, i.e. egoism or whatever values an agent already has.
Specifically, (multiverse-wide) superrational compromise can be viewed
as everyone switching to a new utility function, but only because it
benefits their current utility function.

There are many other examples of agents effectively adopting a new goal.
Consider an egoist living in 16th-century Spain. Her environment
punishes people who are not aligned with Catholicism. To further her
goals, the egoist should therefore behave as though she was a Catholic
with pure Catholic goals. She thus derives a new ``morality'' from
purely egoistic goals, but I suspect that people's excitement about this
would be limited.

\subsubsection{How much altruistic behavior does superrationality
entail?}\label{how-much-altruistic-behavior-does-superrationality-entail}

The second issue is that superrationality does not suffice for
reproducing all of our moral intuitions. For one, I am not sure to what
extent superrationality has a bearing on interactions with other people
on Earth at all (see section
\protect\hyperlink{_hbp3s2xblhpq}{\emph{Superrational compromise on
Earth}}).

Furthermore, we saw that superrationality only warrants helping other
superrational agents (see section
\ref{only-helping-superrational-cooperators-helps-you-superrationally}). But
our moral intuitions also regard other agents as morally relevant. As an
example, consider Alice, a purely causal decision theorist who even
defects in a prisoner's dilemma against her copy. Does this mean that
Alice is morally irrelevant, no matter her degree of consciousness,
capacity to suffer, etc.? Alice is not just a thought experiment --most
philosophers would two-box in Newcomb's problem (see section
\ref{a-short-survey-of-decision-theories-and-their-relation-to-superrationality}). Since Newcomb's problem is roughly equivalent to
the prisoner's dilemma against an identical copy
(\href{https://sl4librarian.files.wordpress.com/2017/01/lewis-prisoners-dilemma-newcomb-problem.pdf}{\emph{Lewis
1979}}), this shows that most philosophers reject superrationality.
Nevertheless, I and presumably most others care intrinsically about the
welfare of these moral philosophers; the same is true for young children
and non-human animals, most or all of which do not reason
superrationally. Superrationality and what we would usually call
``morality'' thus disagree strongly on who is morally relevant
\parencite{Drescher2006-ky}.

\hypertarget{multiverse-wide-superrationality-for-causal-decision-theorists}{\subsection{Multiverse-wide
superrationality for causal decision
theorists}\label{multiverse-wide-superrationality-for-causal-decision-theorists}}

Throughout this paper, I have assumed that some \emph{acausal} decision
theory is correct, albeit without narrowing it down to any particular
theory. To me, this is no limitation of MSR, because I hold that
\emph{causal} decision theories fail in examples like the donation game
with similarity. However, many professional philosophers are causal
decision theorists (see
\ref{a-short-survey-of-decision-theories-and-their-relation-to-superrationality}). Are the arguments presented in this paper
entirely irrelevant to them?\footnote{One obvious way in which the
  implications are relevant to causal decision theorists is
  decision-theoretical uncertainty
  \parencite{MacAskill2016-zo}. Perhaps, even ardent
  defenders of CDT have probability on CDT being the wrong way to make
  decisions. I, at least, do not have a probability of 100\% on a single
  decision theory being the right one. If you have some weight on some
  of the alternatives to causal decision theory, then you would also
  give MSR considerations some weight. In fact, Treutlein (unpublished,
  TODOLaTeX) argues that if we live in a sufficiently large universe,
  then EDT and other non-causal decision theories immediately dominate
  expected value calculations that take decision-theoretical uncertainty
  into account.}

Remember, from section
\ref{cdt-would-self-modify-to-behave-like-a-non-causal-decision-theory-in-some-newcomb-like-problems}, that CDT actually recognizes its flaw. Specifically,
CDT self-modifies to cooperate acausally with copies that are created in
the future. After all, these copies can be \emph{causally} influenced to
cooperate acausally to each other's benefit. Other humans and
extraterrestrials in far away parts of the multiverse do not fall into
that category, of course -- so causal decision theorists would not
precommit to engage in full multiverse-wide superrational cooperation.

However, one multiverse theory is the Everett interpretation of quantum
physics, according to which our universe constantly ``splits'' into
different branches. Thus, under the Everett interpretation, near-copies
of oneself are created all the time and in large quantities.

Moreover, it pays in causal terms to cooperate across time, i.e. to
commit me\textsubscript{tomorrow} and me\textsubscript{in-30-years} to
cooperate. A causal decision theorist would therefore cooperate with a
large number of agents created after CDT's precommitment. It thus seems
as though a weaker version of the considerations from this paper apply
to causal decision theorists after all.

\hypertarget{simulations}{\subsection{Simulations}\label{simulations}}

Paul Almond
(\href{https://web.archive.org/web/20120310010225/http://www.paul-almond.com/Correlation2.pdf}{\emph{2010b}},
ch. 4.2;
\href{https://www.researchgate.net/publication/265224117_Can_you_retroactively_put_yourself_in_a_computer_simulation}{\emph{2010c}})
has argued that correlations across the multiverse have implications for
whether and how we should simulate other civilizations. The idea has
also been proposed by others. It is mainly relevant for agents and
civilizations who primarily care about copies of themselves, which it is
not discussed in the main text.

\subsubsection{If being in a simulation is bad, avoid creating
one}\label{if-being-in-a-simulation-is-bad-avoid-creating-one}

Almond
(\href{https://web.archive.org/web/20120310010225/http://www.paul-almond.com/Correlation2.pdf}{\emph{2010b}},
section 4.2) writes:

\begin{quote}
If you take the
\href{http://www.simulation-argument.com/}{\emph{simulation argument}}
seriously, then evidential decision theory would seem to allow you to
assert some control over the other civilizations that might be building
these simulated realities.

One way in which evidential decision theory would be relevant is in the
way it allows you to control the probability that you are in a
simulation in the first place. If your civilization decides to develop
the capability to run simulated realities, then you are meta-causing
{[}i.e. influence acausally{]} civilizations in general to do likewise
(including civilizations on which our own might be modeled), and making
it less likely that almost all civilizations end before they are capable
of producing simulated realities, in turn making it more likely that you
are in a simulated reality. If, however, your civilization decides not
to acquire this capability then you are meta-causing civilizations in
general to do likewise, making it less likely that you are in a
simulated reality. Once your civilization has the capability to produce
simulated realities, if your civilization decides to do it, this would
make it more likely that other civilizations also do it, again making it
more likely that you are in a simulated reality. On the other hand, if
your civilization decides not to produce simulated realities, this makes
it less likely that other civilizations would choose to do so, and
therefore less likely that you are in a simulated reality yourself.
\end{quote}

If you assume the view of
\href{https://www.youtube.com/watch?v=aiGOGkBiWEo}{\emph{anthropic
decision theory}}
(\href{https://arxiv.org/abs/1110.6437}{\emph{Armstrong 2011}}) instead
of classical anthropics (i.e., the
\href{https://en.wikipedia.org/wiki/Self-sampling_assumption}{\emph{self-sampling}}
or
\href{https://en.wikipedia.org/wiki/Self-indication_assumption}{\emph{self-indication}}
assumption), then your decision can affect the fraction of copies of you
that are in a given simulation.

Note that under certain assumptions about the efficiency of simulations,
one's effect on the probability of being in a simulation may be
negligible. If any civilization could run orders of magnitudes more
simulations of civilizations than there are civilizations in the
basement, then most copies will be in simulations no matter what you
decide. Regardless of your choice, you will probably be in a simulation.

\subsubsection{Happy simulations}\label{happy-simulations}

Almond
(\href{https://web.archive.org/web/20120310010225/http://www.paul-almond.com/Correlation2.pdf}{\emph{2010b}},
section 4.2) proposes to simulate civilizations in a nice way to
increase the probability of being in such a simulation oneself.

\begin{quote}
While evidential decision theory might be applied to try to reduce your
``risk'' of being in a simulated reality, some people, and some
civilizations, might not see it that way: They might think that being in
a simulated reality could have benefits if the entity that constructed
the simulation is kind; for example, the inhabitants of the simulation
might be protected from existential risks to their civilization, or they
might be provided with an afterlife. Evidential decision theory suggests
the possible tactic of making large numbers of simulated realities in
which the inhabitants are treated kindly as a way of trying to
meta-cause civilizations in general to do the same thing. This would be
going further than what I said previously about treating the inhabitants
of your own simulations kindly: This would be done so as to make it more
likely that you are in a simulation, and that it is one in which you
will be treated kindly. We might imagine a civilization doing this as a
way of trying to use evidential decision theory to pluck an afterlife
out of nowhere for itself, if it has recently acquired the computing
power to simulate many civilizations, and provide them with an
afterlife, but does not yet have technology such as mind uploading which
it might use to obtain an afterlife more directly. A civilization might
attempt this even if it does not yet have the computing power to
construct simulated realities: It might set up some kind of legal or
corporate framework to ensure that large numbers of ancestor
simulations, complete with an afterlife, are constructed in the future,
the idea being to strengthen the case that it is itself in such a
simulation, made by a civilization with a past that is strongly
correlated with its own present. Someone might even set up some
organization for this purpose as a result of reading this article!
\end{quote}

\hypertarget{infinite-ethics}{\subsection{Infinite
ethics}\label{infinite-ethics}}

In all our calculations (sections
\ref{the-relative-importance-of-superrational-cooperation-an-example-calculation} and
\protect\hyperlink{_8pf5otrvp6f4}{\emph{Fair divisions of gains from compromise}}) we assume finite
numbers of agents each with a finite causal influence on their world. However, the multiverse -- or
even a single universe -- may well be infinite. These infinities entail severe complications for the
application of multiverse-wide consequentialist moral views like those required for
multiverse-wide superrational cooperation
(\href{http://www.nickbostrom.com/ethics/infinite.pdf}{\emph{Bostrom
2011;}}
\href{http://lukemuehlhauser.com/wp-content/uploads/Arntzenius-Utilitarianism-decision-theory-and-eternity.pdf}{\emph{Arntzenius
2014)}}. Superrationality is a form of what Bostrom
(\href{http://www.nickbostrom.com/ethics/infinite.pdf}{\emph{2011}}, ch.
4.6) calls ``class action'': through our actions, we can
\emph{acausally} affect an infinite amount of value, even if each
physical instantiation of ourselves only has a finite \emph{causal}
impact. It seems unclear whether this makes infinite ethics even more
challenging, or whether it can be viewed as a step toward a solution
(cf.
\href{https://web.archive.org/web/20120310010225/http://www.paul-almond.com/Correlation2.pdf}{\emph{Almond
2010b}}, ch. 3.2). One's preferred approach to the problem of infinite
ethics may well be consequential for a variety of issues (including
MSR), which is why FRI
\href{https://foundational-research.org/infinity-in-ethics/}{\emph{lists
infinite ethics}} as a promising area for future research. Nonetheless,
I expect a solution to preserve most of the conclusions drawn from
traditional (i.e. finite) ethics.

\hypertarget{objection-based-on-uncertainty-about-the-values-of-superrationalists-in-the-multiverse}{\subsection{Objection
based on uncertainty about the values of superrationalists in the
multiverse}\label{objection-based-on-uncertainty-about-the-values-of-superrationalists-in-the-multiverse}}

Thoughts on the value systems of extraterrestrials are necessarily
speculative and uncertain. At what level of certainty about some other
value system should we invest resources into maximizing it? Indeed, one
possible criticism of MSR is that we will never be sufficiently certain
of just how common some other value system is. Thus, the argument goes,
we should in practice never take any specific value systems into
consideration.

First note that superrationality is still relevant even if you do not
know the other value systems. There are some interventions that benefit
other superrationalists without requiring knowledge of their values (see
section
\ref{cooperation-in-the-face-of-uncertainty-about-values}), such as
making future superintelligent AIs cooperate superrationally (under the
assumption that they will come to understand the values of other agents
in the multiverse much better than we do).

But even if the argument acknowledges this, it is still invalid,
primarily because it ignores the fact that we do not know how common our
\emph{own} value system is, either. In section
\ref{compromise-strategy},''
we argued that if we consider the correlations between our actions and
the behavior of agents elsewhere in the multiverse, then maximizing a
neutral compromise utility function in our local universe maximizes our
original utility function in the multiverse at large. This argument also
applies if we are uncertain about the other agents' utility functions
and thus the compromise utility function itself. Thus, it must be
possible to state the criticism in terms of the compromise utility
function. For example, the criticism may translate to the following
statement: the only terms in the compromise utility function that we can
be certain about represent our own values. We are so uncertain about all
other value systems that they do not contribute much to estimates of
compromise utility. This criticism could, in theory, be true. Imagine
you grew up on a planet where everyone had the same value system as
yours; even if you believed that the universe also has other value
systems, you would be justified not to assign much weight to any other
specific value system. On Earth, however, we already observe quite some
variety in what people care about. Thus, no matter what value system you
hold, there are probably other value systems that are similarly common
on Earth. Of course, we still do not know whether these value systems
are also common elsewhere in the universe, but your own value system is
a priori not in a privileged position that would justify assuming it to
be more common than others. Solely maximizing our own utility function
in this universe thus seems to be a bad approach towards maximizing the
compromise utility function, in turn making it suboptimal in terms of
our multiverse-wide utility.
