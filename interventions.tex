In this section, I propose a few interventions that multiverse-wide
superrationalists should pursue. Many of these are tentative ideas to
consider more in the future rather than confident suggestions for what
we should do today.

\hypertarget{cooperation-in-the-face-of-uncertainty-about-values}{\subsection{Cooperation
in the face of uncertainty about
values}\label{cooperation-in-the-face-of-uncertainty-about-values}}

We begin with a general challenge: given that we currently know so
little about the values of other agents in the multiverse, how can we
cooperate with them? With our current state of knowledge, it appears
impossible to conclude what position MSR recommends on particular
issues. For example, it seems impractical to decide whether we should
vote and lobby in favor or against mass surveillance, abortion,
marijuana legalization or the death penalty. Perhaps MSR, while
interesting in theory, is practically impossible to apply because of our
ignorance of the values in the multiverse? (Also see section
\ref{objection-based-on-uncertainty-about-the-values-of-superrationalists-in-the-multiverse}.) While our uncertainty about the values of our
collaborators is no doubt a major obstacle to the application of MSR, I
will nonetheless argue that there are relevant policy changes that we
can implement even today.

The first class of such interventions requires no knowledge about other
value systems at all, as long as we are confident that future agents
will be able to attain such knowledge. Meta-activities are examples of
this: no matter what the aggregated utility function of all
superrationalists in the multiverse turns out to be, we could still
benefit it indirectly by learning what it is or by spreading MSR itself
(see section
\ref{meta-activities}). One
way of doing so is to ensure that artificial intelligences cooperate
superrationally (see section
\ref{artificial-intelligence}).

In the second class of feasible interventions, we try to draw
conclusions from what little we do know about the distribution of values
in the multiverse. We can, for instance, be sure that extraterrestrial
will care less than humans about the Bible or the United States of
America (though some will care about them a lot and many may care about
preserving local traditions in general). On the other hand, we can be
reasonably confident that many extraterrestrials care about satisfying
the preferences of some other agents (e.g., ``innocent'' agents capable
of reciprocating) (see, e.g.,
\cite{Axelrod2006-ci,Trivers1971-rb,Fehr1999-pd,Dawkins1976-cd,Taylor1987-wn,Buss2015-kp}).
Hence, we should perhaps embrace such ``universal'' moral values more
than human superrationalists would otherwise do. (We explore this
further in section
\ref{universalism}.) Consider
another example: the far values of at least some humans probably
resemble those of many evolved extraterrestrial superrationalists, which
means that we can benefit our superrationalist collaborators by
increasing the capabilities of these humans to fulfill these preferences
(see section
\ref{increasing-capabilities}). As a last example of how we can use a small piece of
knowledge, consider how we can sometimes know that someone's values are
at an extreme end of some scale or otherwise far away from the
multiverse-wide superrationalist average. In this case, MSR suggests
that we shift these extreme values towards the middle of their scale.
For example, utilitarians are extreme in that they only care about
welfare, whereas most superrationalists presumably care about a lot of
other things as well. Thus, it would be good to convince the utilitarian
to take other considerations into account (although it is not clear what
these ought to be and how much they should be taken into account).

In both of these classes, we overcome the obstacle posed by our lack of
knowledge by benefitting a wide variety of value systems, rather than
picking out any particular subset of extraterrestrials.

\hypertarget{universalism}{\subsubsection{Universalism}\label{universalism}}

I think satisfying universalist values, i.e. ones that are shared by a
large fraction of superrationalists, may become somewhat more important
for all superrationalists, although the case is not entirely clear.

Imagine a group of people with a few shared concerns, such as justice,
welfare and freedom, and a large number of non-shared concerns, such as
each person's egoism, tribal loyalties, etc.\footnote{Note that the
  distinction between universal and idiosyncratic concerns is not
  binary. For example, consider I would guess that valuing
  \href{https://en.wikipedia.org/wiki/Eternal_flame}{\emph{eternal
  flames}} is much more common in the multiverse than most religions and
  tribal loyalties but less common than concern for justice, welfare and
  freedom.} Given this, they can produce gains from trade by moving
resources from the non-shared concerns to the shared concerns. In terms
of the compromise utility function (see section
\ref{compromise-strategy}), the idiosyncratic concerns do not receive lower
collective weight than prior to cooperation. However, since each
individual idiosyncratic value receives much smaller weight in the
compromise utility function and interventions usually cannot satisfy
many of them at the same time, interventions targeted at the universal
concerns will usually increase the compromise utility function more
efficiently.

Although this argument is quite persuasive, it is not as strong as it
initially seems. For example, it assumes that each individual's values
are a simple weighted sum of universal and idiosyncratic concerns. But
preferences can also have different shapes. In fact, each agent may
explicitly protect its idiosyncratic values against losing its weight in
such a preference aggregation mechanism. One example could be that the
idiosyncratic values are much stronger than other preferences, but face
diminishing returns. For instance, most people probably care almost
exclusively about their own well-being until their level of well-being
reaches some threshold.

There may also be agents who exclusively care about their idiosyncratic
preferences. For agents with these values, a compromise in which
resources shift to universal concerns is negative.

Another reason to disregard idiosyncratic preferences is that they are
often not more common than their opposites. For example, Marxism, the US
or Islam are liked by many, but also disliked by many others. Therefore,
it is not even clear whether the compromise utility function evaluates
either one of them positively.

It should be noted that some universalist values may refer to others'
tribal values. For example, many humans care about preserving
\href{https://en.wikipedia.org/wiki/Cultural_heritage}{\emph{cultural
heritage}}. That said, this preference is usually weak and usually
abandoned if it conflicts with other values. For instance, few would
argue that human sacrifices should be continued to preserve tradition.
Although most people do not care enough about animals to become
vegetarians, my impression is that most people in Western countries
would favor the abolition of bullfighting.

\hypertarget{moral-advocacy}{\subsection{Moral
advocacy}\label{moral-advocacy}}

Advocating one's moral views
\href{http://www.utilitarian-essays.com/values-spreading.html}{\emph{can
be}} an effective intervention if they differ significantly from those
of most other people. In light of superrational cooperation, we should
perhaps change the values we advocate.

\hypertarget{universalist-values}{\subsubsection{Universalist
values}\label{universalist-values}}

As I argued in section
\ref{universalism}, I think that
superrational compromise utility function may imply that more resources
are used to satisfy universal rather than idiosyncratic concerns. This
suggest that spreading universalism is good from an MSR perspective.

\paragraph{Expanding the moral circle}\label{expanding-the-moral-circle}

Most people care much more about themselves, their kin, and their
friends than about most others. From their point of view, they, their
kin, and their friends are all special. From the outside view, however,
most people are not more important than others. It is thus in the
benefit of altruistic outsiders (e.g., other humans) to reduce the
difference between how much people care about themselves, their family,
friends, etc. versus other humans. In Singer
terminology, an outsider who cares
about all humans equally would similarly want people's ``circle of
empathy'' to expand outwards to include other humans \parencite{Singer2011-yx}. In this way, we
can align their decisions with the goals of the outside party.

The perspective of superrational collaborators elsewhere in the
multiverse is similar, in that many things that are morally special to
us are not special to them. Take
\href{https://en.wikipedia.org/wiki/Nationalism}{\emph{nationalism}} and
\href{https://en.wikipedia.org/wiki/Patriotism}{\emph{patriotism}}: many
people assign particular moral value to the country they grew up in or
to its citizens, with little support through impartial reasons\footnote{Surely,
  there are some impartial reasons to like one country more than
  another. For instance, Sweden is more tolerant of homosexuals than
  Iran, which is a reason to favor Sweden if one cares about the welfare
  of homosexuals. Nationalists often provide impartial reasons for
  favoring their country. For example, US nationalism is often about how
  the US is the country with the most freedom in the world. But if
  people really cared about such impartial reasons, the ``best country
  in the world'' would usually not be their own country. (Because values
  and therefore criteria of judgment differ among cultures, the ``best
  country'' estimates would still diverge, though.) Furthermore,
  nationalism often exaggerates the difference between countries in a
  way that seems inconsistent with an impartial point of view: sure, the
  US has a lot of freedom, but so does Canada, most of Western and
  Northern Europe, Australia and New Zealand, and so on. If the US is
  better along such dimensions at all, then surely not by a big margin.
  In any case, the given paragraph is only about the kind of nationalism
  that is not based on impartial arguments.}. Needless to say, most
superrational collaborators elsewhere in the multiverse will adopt a
different perspective. If they care more about Japan than about the
United States (or vice versa), it would be for specific impartial
reasons. Making people care intrinsically less about particular nations
thus aligns their values more with those of superrational collaborators
elsewhere in the multiverse. Similarly, intrinsic preferences for
members of one's
\href{https://en.wikipedia.org/wiki/Racism}{\emph{race}},
\href{https://en.wikipedia.org/wiki/Speciesism}{\emph{species}}, or
substrate are inconsistent with an outside view of someone from a
completely different species with a different substrate.

\paragraph{Which moral foundations?}\label{which-moral-foundations}

Given the criterion of universalism, what aspects of morality are worth
spreading? As an illustrative classification of moral intuitions, we use
Haidt's
\href{https://en.wikipedia.org/wiki/Moral_foundations_theory}{\emph{moral
foundations theory}}, which divides morality up into five foundations:
care/harm, fairness/cheating, loyalty/betrayal, authority/subversion and
sanctity/degradation (see section
\ref{organizing-human-values}). Liberals tend to care primarily about the first two
aspects, whereas conservatives care about all five.

Liberal values are universalist, while the exclusively conservative
values are not. As Greene writes \parencite{Greene2013-sq}) (links
added from the endnotes):

\begin{quote}
According to Haidt, American social conservatives place greater value on
respect for authority, and that's true in a sense. Social conservatives
feel less comfortable slapping their fathers, even as a joke, and so on.
But social conservatives do not respect authority in a \emph{general}
way. Rather, they have great respect for the authorities recognized
\emph{by their tribe} (from the Christian God to various religious and
political leaders to parents). American social conservatives are not
especially respectful of Barack Hussein Obama, whose status as a
native-born American, and thus a legitimate president, they have
persistently challenged. {[}...{]} Likewise, Republicans, as compared
with Democrats and independents,
\href{http://www.pewglobal.org/2009/09/21/obama-addresses-more-popular-un/}{\emph{have
little respect for the authority of the United Nations}}, and a majority
of Republicans say that a Muslim American with a position of authority
in the U.S. government should not be trusted
(\href{http://b.3cdn.net/aai/3e05a493869e6b44b0_76m6iyjon.pdf}{\emph{Arab
American Institute 2014}}). In other words, social conservatives'
respect for authority is deeply tribal, as is their concern for
sanctity. (If the Prophet Muhammad is sacred to you, you shouldn't be in
power.) Finally, and most transparently, American social conservatives'
concern for loyalty is also tribal. They don't think that
\emph{everyone} should be loyal to their respective countries. If
Iranians, for example, want to protest against their government,
\href{http://www.cbsnews.com/news/gop-hits-obama-for-silence-on-iran-protests/}{\emph{that
is to be encouraged}}.
\end{quote}

In other words: authority, loyalty, and sanctity are all
non-universalist values. While many people have values that structurally
fit into these categories, the content (e.g., the
\href{https://en.wikipedia.org/wiki/Referent}{\emph{referent}}) of these
values differ. Applied to multiverse-wide superrational cooperation,
this means that we cannot benefit the authority, loyalty, and sanctity
values of other superrationalists unless we are in a society with the
``right'' authorities and sanctity rules. In fact, if we push for these
three values in our tribe (or civilizations), it may actually be
\emph{bad} from the perspective of people with conservative values from
other tribes. American social conservatives tend to dislike Islam and
loyalty to its authorities, even more than American liberals do.
Overall, this suggests that when it comes to multiverse-wide compromise,
spreading values in the domains of authority, loyalty, and sanctity is
not very fruitful. Instead, we should try to make people care more about
the universalist liberal foundations.

Having said this, there may be a few exceptions to the rule (cf. the
last paragraph section \ref{universalism}). For example,
Christian social conservatives may like parental authority even if one's
parents are Muslims or extraterrestrials. In the sanctity domain, a
preference for leaving nature untouched may extend beyond an agent's
planet, although many extraterrestrial habitats are probably ``slimy''
and full of scary animals. Presumably, such reasoning is also applicable
to other moral values. For instance, some people care about the
traditions of other tribes, including their art, social institutions,
laws, religions and other non-universal aspects.

Another reason to focus on the liberal aspects of morality is that
potential superrationalists on Earth are rarely conservative (see
section \ref{effective-altruists}). That said, future societal transitions might make people
more conservative (see the companion paper on whole brain emulation
(TODOlater)).

\subsubsection{Concern for benevolence}\label{concern-for-benevolence}

We have seen provisional research indicating that, when it comes to
distant societies, humans mainly care about the benevolence, warmth, and
moral behavior of its inhabitants (see section
\ref{human-far-values}). If
these tentative findings turn out to be correct and other evolved
species resemble ours in this regard, we should try to align people's
near values more with these (typically far) goals. However, given the
tentativeness of said research, I do not think this should significantly
affect our actions at present.

\subsubsection{Consequentialism}\label{consequentialism-1}

Even though superrationalists elsewhere in the multiverse may care most
about whether we behave in a non-consequentialist but broadly ethical
way, they do so in a consequentialist way (see section
\ref{consequentialism}). For
example, they might care about the numbers of crimes and selfless acts,
or total amounts of happiness and suffering in a given population. This
stands in contrast to the preferences revealed by most people's
charitable efforts: \href{http://www.givewell.org/giving101}{\emph{most
money is donated to charities that are comparably ineffective}}, i.e.
ones that do not achieve the best possible consequences. By making
people more consequentialist, we can improve their resource use from the
perspective of consequentialist third parties. This suggests that we
should spread consequentialist ideologies like
\href{https://en.wikipedia.org/wiki/Effective_altruism}{\emph{effective
altruism}}, potentially independently of any particular optimization
target (such as injustice, suffering, happiness, or knowledge).

\hypertarget{pluralism}{\subsubsection{Pluralism}\label{pluralism}}

Whereas the compromise utility function incorporates a plethora of
concerns, most individuals' values are much more narrow. This is
especially true among people who give morality some thought. For
example, some people adopt utilitarianism, while others become
proponents of
\href{https://en.wikipedia.org/wiki/Categorical_imperative}{\emph{Kant's
categorical imperative}}.\footnote{That said, advocates of simple
  ethical views like utilitarianism often argue that the implications
  resemble other ethical notions. For example, because receiving an
  additional unit of resources
  \href{https://80000hours.org/articles/money-and-happiness/}{\emph{has}}
  a greater impact on a poor than a rich person's happiness,
  utilitarianism tends to prefer an even distribution of resources
  \parencite{noauthor_2012-yb}. Similarly, it has been
  argued that utilitarianism is (often) consistent with the wrongness of
  killing, justice \parencite{Mill1863-bm} and other moral
  rules \parencite{Smart1973-eo}. This decreases the value
  of making utilitarians more pluralistic. It should be noted, however,
  that many (especially critics of utilitarianism) have argued for the
  opposite, i.e. that there are some moral intuitions that
  utilitarianism cannot make sense of
  \parencite{noauthor_undated-mj}. For example, they argue
  that utilitarianism is not (always) consistent with moral intuitions
  about equality \parencite{Pogge1995-mo,Gosepath2011-vx},
  the wrongness of killing \parencite{Henson1971-le}, and
  justice \parencite{Smart1973-eo}.}

As I am primarily a utilitarian, I sympathize with adopting a single
ethical view (and utilitarianism in particular). From an MSR
perspective, on the other hand, this misses out on gains from compromise
between these opposing value systems and it would be better if everyone
adopted a mix of different values instead. Thus, we may want to promote
moral pluralism.

One version of this view is MacAskills' 
moral uncertainty \parencite{MacAskill2014-ca}. Operating under the assumption of moral realism
(which I reject), he argues that and how we should be uncertain about
which ethical system is correct. Another related view is the normative
reading of Yudkowsky's
\href{https://wiki.lesswrong.com/wiki/Complexity_of_value}{\emph{complexity
of value}} (cf. \cite{Stewart-Williams2015-gg}, section
``Morality Is a Mess''; \cite{Muehlhauser2012-ib}),
according to which what humans care about cannot be captured by a simple
moral system and instead incorporates a large number of different
values.

\subsubsection{Promoting moral
reflection}\label{promoting-moral-reflection}

Probably wanting more idealized and reflected upon values to be
implemented is much more common in the multiverse than wanting less
idealized values to be implemented.\footnote{The main data point is that
  humans think about morality and engage with others' moral views. The
  evolutionary psychology and cultural evolution perspectives, on the
  other hand, are non-obvious. Some moral arguments may be favored by
  \href{https://en.wikipedia.org/wiki/Cultural_group_selection}{\emph{cultural
  group selection}}, others may offer intelligent individuals to get
  their way more often. On the other hand, individuals who change their
  moral views may be perceived as unreliable or illoyal.} This is
especially the case for agents who have not yet settled on a moral view.
For example, I am genuinely uncertain about what I would or should count
as morally relevant suffering when it comes to small minds (such as
those of insects) and the like, just as I am not sure how to deal with
\href{https://foundational-research.org/infinity-in-ethics/}{\emph{infinities
in ethics}}. I could thus benefit a lot if someone were to make more
people think about these problems.

Interestingly, the appeal of promoting moral reflection decreases upon
idealization. Most people probably endorse moral discourse, the
importance of reflection and argument, etc., in part because they think
\emph{their} moral view will result from that process -- if they did not
believe they had the arguments on their side, they might not hold their
moral position in the first place. However, not everyone can be right
about this at the same time. If someone only cares about preference
idealization because she thinks that her value system will win, then
preference idealization may remove that meta-preference.

Beyond the question of whether evolved agents in the multiverse care
about moral discourse, we must ask an empirical question about our own
universe: will moral discourse bring people's object-level positions
closer to those of our multiverse-wide superrational compromise utility
function? For example, does moral discourse make people care more about,
say, benevolence, assuming this really turn out to characterize much of
evolved agents' far values (see section
\ref{human-far-values})?
Perhaps moral reflection can also have negative consequences as well,
such as
\href{https://en.wikipedia.org/wiki/Group_polarization\#Attitude_polarization}{\emph{attitude
polarization}} \parencite{Lord1979-sc,Taber2006-ew}. These
questions appear suitable for further research.

Besides promoting societal discourse on ethical questions, one
intervention in this domain is the use of preference idealization in
artificial intelligence value loading (see section
\ref{artificial-intelligence}).

\hypertarget{multiverse-wide-preference-utilitarianism}{\subsubsection{Multiverse-wide
preference
utilitarianism}\label{multiverse-wide-preference-utilitarianism}}

In addition to spreading MSR itself, one could also spread value systems
that in some way mimic its implications. Specifically, the proposed
neutral aggregated utility compromise (see section
\protect\hyperlink{_2uwv44pwn55u}{\emph{Aggregated utility}}) is
essentially a form of
\href{https://foundational-research.org/hedonistic-vs-preference-utilitarianism/}{\emph{preference
utilitarianism}} or
\href{http://lesswrong.com/lw/jll/multiversewide_preference_utilitarianism/}{\emph{multiverse-wide
preference utilitarianism}}. Multiverse-wide preference utilitarianism
might therefore be a promising moral view to advocate on the basis of
multiverse-wide superrational compromise.

Of course, spreading a proxy for MSR has some general disadvantages.
Most importantly, it is not very robust. If multiverse-wide preference
utilitarians come to prioritize very differently than multiverse-wide
superrationalists, then spreading the preference utilitarianism would
not yield much in our favor. The question nonetheless deserves some
thought. After all, if there is a significant chance that
multiverse-wide preference utilitarianism approximates the conclusions
of MSR, then we should at least be on the lookout for very cheap ways of
promoting it.

One main difference between preference utilitarianism and superrational
cooperation -- whether in the form of aggregated utility compromise or
otherwise -- is that the latter only takes the values of other
superrationalists in the multiverse into account (see section
\ref{only-helping-superrational-cooperators-helps-you-superrationally}).
Preference utilitarianism, on the other hand, accounts for the
preferences of a much broader set of agents, such as all sentient
beings, all agents that have preferences of any sort, or all agents who
satisfy some other criteria for
\href{https://en.wikipedia.org/wiki/Personhood}{\emph{personhood}}. This
may mean that preference utilitarians arrive at very different
conclusions than MSR proponents. For example, if they take small minds
into account, these
\href{https://en.wikipedia.org/wiki/Biomass_(ecology)\#Global_biomass}{\emph{may
well}} dominate preference aggregation. If, on the other hand, they only
take members of human-like species into account, then the difference
between these and superrationalist preferences may be much smaller.

Another difference could be the way interpersonal comparison of utility
is handled (cf. section
\ref{how-to-assign-the-weights}). In the context of compromise, an individual's interests
are usually given weight in proportion to the individual's power. So,
for example, the interests of a superrational billionaire receive orders
of magnitude more weight than the interests of a superrational beggar.
However, most would view this approach as unethical and most preference
utilitarians would disagree with it. Thus, multiverse-wide preference
utilitarianism gives more weight to the moral views of the poor than MSR
suggests.

Yet another problem could be that preference utilitarians would not
arrive at the more meta-level MSR interventions. Even if MSR and
multiverse-wide preference utilitarianism had the same object-level
implications, the justification for MSR is different from (non-MSR)
justifications for preference utilitarianism. Thus, preference
utilitarians would not support or even come up with interventions that
are about spreading the MSR-based justifications for MSR's and
preference utilitarianism's joint conclusions. For example, a preference
utilitarian (who does not agree with MSR) would not spread the MSR idea
itself, nor try to ensure that future people (and AIs, see section
\ref{making-an-ai-come-up-with-superrational-cooperation-on-its-own}) reason
correctly about decision theory. Because these are plausibly among the
most promising interventions, this consideration suggests some
significant divergence in priorities.

In sum, it is unclear to what extent multiverse-wide preference
utilitarianism could approximate a superrational compromise utility
function. At this point, however, spreading multiverse-wide preference
utilitarianism is unlikely to be a top priority.

\subsubsection{No multiverse-wide tug-of-war over
values}\label{no-multiverse-wide-tug-of-war-over-values}

Value systems can be viewed as having several dimensions, like relative
importance of welfare, population size, art, knowledge, justice,
compassion and freedom, tradeoffs between suffering and happiness,
tradeoffs between extreme happiness/suffering and mild
happiness/suffering, and severity of punishments, to name but a few.
Different groups in the multiverse invest resources into pulling the
relative values of these dimensions into different directions. Some may
want people to care more about suffering, while others want them to care
more about nature or happiness instead.

Now, imagine you care more about suffering than most others and that you
live in a civilization with a merely average concern for suffering.
Presumably, you would want to pull the ``concern for suffering rope''
into your direction, potentially at the cost of other values. But
knowing about superrationality, this would make it more likely that
those who care less than average about suffering will also pull the rope
into \emph{their} direction elsewhere in the multiverse, thus offsetting
your impact. Therefore, MSR would recommend against shifting concern
away from other superrationalists' values, e.g., nature or happiness, to
suffering.

It should be noted that the above does not (necessarily) apply if the
values of your civilizations strongly diverge from those of the
superrationalist average far values. In such cases, it may be somewhat
beneficial if all superrationalists pull the values of their
civilization toward the average.

\subsection{Promoting causal
cooperation}\label{promoting-causal-cooperation}

Imagine two value systems, each of them common throughout the
multiverse, engaged in conflicts with one another on Earth. Let us also
assume that most people with these value systems find ideas like acausal
decision theory and the multiverse highly speculative, such that we
cannot convince them of cooperating on a MSR basis. In this case, we can
still cooperate superrationally with others in the multiverse by
promoting \emph{causal} cooperation between the two sides (provided this
does not end up hurting some third superrational party of
agents\footnote{As a non-obvious example, consider global catastrophic
  risks. Presumably, most people would not want humanity to experience a
  global catastrophe. Promoting peace and cooperation between nuclear
  powers is thus positive for all nuclear powers involved. In the
  plausible event that humanity would survive a nuclear winter and
  quickly recover, however, post-apocalyptic human society
  \href{https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/\#Other_costs_to_catastrophes}{\emph{may}}
  come to hold different moral views that conflict with the views of
  current nuclear powers. For instance, it may be that in the first
  months after a global catastrophe, there would be frequent violence
  and chaos among survivors. They may also be forced to exert violence
  themselves to survive. Thus, the survivors may be desensitized to
  violence. Even after civil order is reestablished, citizens may still
  be relatively unconcerned about violence towards animals, criminals,
  the weak and poor, etc. (Note that I am not claiming that this would
  \emph{necessarily} be the case; indeed, personal hardships
  \href{http://reducing-suffering.org/how-important-is-experiencing-suffering-for-caring-about-suffering/}{\emph{can}}
  also make people \emph{more} compassionate. I am merely using it as a
  somewhat plausible scenario to illustrate the present point.) All of
  this would imply that mitigating global catastrophic risks on Earth
  ends up hurting agents in the multiverse who would like societies to
  be organized according to post-apocalyptic survivor values. If agents
  with such values are sufficiently common in the multiverse, then
  causal cooperation between nuclear powers should actually be
  sabotaged! That said, I do not find this conclusion all that
  plausible. It seems to be based on more and less likely assumptions
  than other action-guiding arguments and so it is much more fragile.
  One specific problem is that I would expect humans (and most other
  evolved beings) to become more tribal in response to a global
  catastrophe \parencite{Henrich2015-xe}, which may make
  these values less important (see section
  \ref{universalist-values}).}).

For example, let us assume that the
\href{https://en.wikipedia.org/wiki/Normal-form_game}{\emph{payoff
matrix}} of their interaction is that of a
\href{https://en.wikipedia.org/wiki/Prisoner\%27s_dilemma}{\emph{prisoner's
dilemma}} given in table \ref{PD-payoff-matrix}. Let us
assume that both players' utility functions are equally common in the
multiverse. We also assume that other value systems have no interest in
the outcome of the interaction. From the perspective of a third party
who accepts MSR, the \emph{effective} payoff matrix for this interaction
may look like the one given in table
\ref{PD-payoff-matrix-superrational-third-party}. That
is, when such a third party can influence the outcome of the interaction
between player 1 and player 2, she acts as though she maximizes the
utilities given in that table, even if she intrinsically cares about
something entirely different. When such an agent is able to influence at
least one of the players, she will lobby him to choose
\emph{C},\footnote{For ideas on promoting cooperation from the outside,
  see Tomasik's
  \href{https://foundational-research.org/possible-ways-to-promote-compromise/}{\emph{Possible
  Ways to Promote Compromise}}, as well as
  \parencite{Axelrod2006-ci}.} because to her, the payoffs
are proportional to the number of \emph{C}'s that are chosen.\footnote{Note
  that in some prisoner's dilemma-like problems, mutual defection is
  overall better than unreciprocated cooperation, in which case the
  superrationalist's job is more difficult. If she convinces one player
  of cooperation but fails to convince the other one, she will have done
  more harm than good.} A disinterested non-superrational third party,
on the other hand, -- i.e. one who does not care about the payoffs of
each of the two agents intrinsically -- would assign no value to either
of the four outcomes, nor would they invest any resources in bringing
about a particular outcome.

\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
    \centering
    \begin{tabular}{p{1cm} p{5mm} p{1cm} p{1cm} }
        & & \multicolumn{2}{c}{player 2} \\
        \cmidrule[.2ex]{3-4}
         & & C &  D\\
         \cmidrule[.1ex]{2-4}
        \multirow{2}{*}{player 1} & C & 2 , 2 & 0 , 3\\
        & D & 3 , 0  & 1 , 1\\
        \cmidrule[.2ex]{2-4}
    \end{tabular}
    \caption{The payoff matrix of a prisoner's dilemma.}
    \label{PD-payoff-matrix}
\end{table}


\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
    \centering
    \begin{tabular}{p{1cm} p{5mm} p{5mm} p{5mm} }
        & & \multicolumn{2}{c}{player 2} \\
        \cmidrule[.2ex]{3-4}
         & & C &  D\\
         \cmidrule[.1ex]{2-4}
        \multirow{2}{*}{player 1} & C & 4&  3\\
        & D & 3 & 2\\
        \cmidrule[.2ex]{2-4}
    \end{tabular}
    \caption{The effective payoffs of a prisoner's dilemma to a third party that cooperates
superrationally.}
    \label{PD-payoff-matrix-superrational-third-party}
\end{table}

Next, let us assume that, rather than some third party, player 1 himself
learns about and adopts multiverse-wide superrational cooperation, while
player 2 stays ignorant of the idea. The new effective payoff matrix may
then look like table
\ref{PD-payoff-matrix-superrational-prisoner}. Player
2's payoffs are the same as in the original prisoner's dilemma, but
player 1's \emph{effective} payoffs have changed. He now maximizes the
sum of the two value systems' payoffs, because player 1's and player 2's
utility functions are equally common in the multiverse. This puts player
1 in a peculiar situation: whereas defection is the dominant strategy in
the original prisoner's dilemma (and therefore still the dominant
strategy for player 2), cooperation dominates in this new
version.\footnote{The same remark as in the previous footnote
  {[}TODOLaTeX{]} applies.} Player 1 would thus cooperate in a one-shot
version of the problem. On Earth, however, most interactions are
repeated, like an
\href{https://en.wikipedia.org/wiki/Prisoner\%27s_dilemma\#The_iterated_prisoner.27s_dilemma}{\emph{iterated
prisoner's dilemma}}. At first glance, one may suspect that player 1
would still cooperate in every round given that, no matter what the
opponent on Earth does, he will want to make it more likely that agents
elsewhere in the multiverse behave in a similarly cooperative way.
However, such a strategy of \emph{unconditional} cooperation makes
defection player 2's best strategy. This is suboptimal for player 1,
given that he prefers mutual cooperation (C,C) over unilateral
cooperation (C,D). In an iterated version of the game, player 1 might
therefore punish defection to some extent, similar to how successful
strategies punish defection in the iterated prisoner's dilemma.
Nevertheless, the dynamics of this new problem are different than those
of the prisoner's dilemma. Based on the ordering of the outcomes for the
different players, the game is identified as \(g_{261}\) or \(g_{266}\)
in the periodic table for 2x2 games by
\href{https://sl4librarian.files.wordpress.com/2016/12/goforthrobinson-the-topology-of-the-2x2-games-a-new-periodic-table.pdf}{\emph{Robinson
and Goforth (2005)}}, who also provide a few examples of games in this
category. A few additional
\href{http://infidels.org/library/modern/mathew/sn-revelation.html}{\emph{examples}}
of this type of game exist, but overall, the game has not been studied
extensively in the literature. Further research is thus needed to
identify the right strategy for iterative versions of the game.

\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
    \centering
    \begin{tabular}{p{1cm} p{5mm} p{1cm} p{1cm} }
        & & \multicolumn{2}{c}{player 2} \\
        \cmidrule[.2ex]{3-4}
         & & C &  D\\
         \cmidrule[.1ex]{2-4}
        \multirow{2}{*}{player 1} & C & 4 , 2 & 3 , 3\\
        & D & 3 , 0  & 2 , 1\\
        \cmidrule[.2ex]{2-4}
    \end{tabular}
    \caption{The effective payoffs
of a prisoner's dilemma, in which player 1 cooperates superrationally
(with extraterrestrial agents who hold player 2's values), but player 2
does not.}
    \label{PD-payoff-matrix-superrational-prisoner}
\end{table}

\hypertarget{increasing-capabilities}{\subsection{Increasing
capabilities}\label{increasing-capabilities}}

Broadly speaking, agents have two reasons to increase other agents'
capabilities: a) they may care about it intrinsically, or b) they may
share the goals of the people whose capabilities they increase (and thus
care about increasing them instrumentally). For example, someone who
mostly cares about other people's freedom to pursue their goals has a
reason to raise the capabilities of poor people for a type a) reason,
and someone who agrees more with the people than with the dictator will
want to increase democracy for a type b) reason. But if you hold less
common values, such as reducing animal suffering, giving people more
power is of unclear value. MSR broadens type b) motives to increase
others' capabilities: even if we do not share someone else's goal, we
have reason to increase his capabilities if we believe that significant
parts of his goals are shared by superrational agents elsewhere in the
multiverse.

There is some relevant literature on increasing an agent's
goal-achievement capabilities.

In economics, the
\href{https://en.wikipedia.org/wiki/Capability_approach}{\emph{capability
approach}} is an alternative to welfare economics and primarily studies
how to measure an individual's capabilities. Some of its metrics include
health, freedom of thought/expression, education, political
participation, and property rights. In his dissertation on
\href{https://rucore.libraries.rutgers.edu/rutgers-lib/34078/pdf/1/}{\emph{Ethics
Under Moral Neutrality (2010)}}, Evan
Gregg Williams discusses a topic closely related to acting
under MSR: he assumes that we do not know what the ``correct'' moral
theory is\footnote{\label{non-cognitivism} I side with moral anti-realism
  \parencite{Joyce2016-no} and
  \href{https://en.wikipedia.org/wiki/Non-cognitivism}{\emph{non-cognitivism}}
  in particular \parencite{Joyce2016-no}. That is, I do not
  think that moral theories can have (objective) truth values.}, and
that while we all have some access to moral truth, it is at best
unreliable \parencite{Williams2011-ul}. He then discusses,
among other things, what policies we should take given such uncertainty.
In many ways, this scenario is analogous to MSR\footnote{In fact, I
  learned about variance voting, which I take to be the most promising
  approach to constructing the compromise utility function, see section
  \ref{how-to-assign-the-weights}, via the literature on moral uncertainty, in
  particular via \parencite{MacAskill2014-ca}.}, where the
necessity to maximize for multiple moral views comes from uncertainty
about the utility functions of other agents in the multiverse as well as
their diversity rather than conflicting intuitions about the ``moral
truth''. Many of Williams' conclusions resemble those of the present
paper. For instance, he identifies the appeal of preference
utilitarianism in chapter 3.1 of the dissertation (compare sections
\protect\hyperlink{_2uwv44pwn55u}{\emph{Aggregated utility}} and
\ref{multiverse-wide-preference-utilitarianism} of the present paper). Many of his
intervention ideas are about improving the capabilities of others who
may plausibly have access to the moral truth. First and foremost, he
defends democracy (chapter 3.3) and liberty (chapte 3.4).

Of course, MSR does not have the same implications as the above
approaches. For one, when we raise others' capabilities as
superrationalists, we favor people whose values we suspect to be typical
of what superrationalists in the multiverse care about. For example,
from an MSR perspective it is much more important to support
consequentialists. Moreover, some of the proposed measures merely move
resources or power from one group to another (e.g., from a dictator to
the people) without adding optimization power aimed at the goals of
superrational agents in the multiverse.

I doubt that raising capabilities will often be a top intervention.
Nonetheless, it might be an option when good and inexpensive
opportunities, such as sharing knowledge, arise.

\hypertarget{meta-activities}{\subsection{Meta-activities}\label{meta-activities}}

Relative to any goal, meta-activities are either about a) amassing more
resources, or b) improving the efficiency of one's object-level resource
expenditure. To achieve the goals that superrationality prescribes, we
may thus also engage in such meta-activities. In the following, I will
describe two meta-activities, one of each kind.

\subsubsection{Research}\label{research}

The present paper lays out the foundations for several areas of research
related to multiverse-wide superrational cooperation. Further research
is needed in all three areas discussed in this paper, i.e. how our new
criterion for choosing policies is to be constructed (see chapter
\ref{superrationality}), what
values our superrational collaborators have (see chapter
\ref{values}), and which interventions
are most promising (chapter
\ref{interventions}).

Note that some research, e.g. investigations of whether a compromise is
beneficial for you, can (in theory) be harmful if one has not properly
precommitted as illustrated in the Remote-controlled cake maker thought
experiment (see section
\ref{updateless-weights}).
A similar danger lies in finding out whether other agents cooperate (see
section
\ref{lack-of-knowledge-is-evidential-power-part-i-the-other-agents}).

\subsubsection{Promoting multiverse-wide superrationality}
\label{promoting-multiverse-wide-superrationality}


Since multiverse-wide superrational cooperation produces gains from
compromise (under certain assumptions about the collaborators, discussed
in section \ref{necessary-preconditions}), having more multiverse-wide superrational
cooperation produces more gains from compromise. Hence, a common
interest of all collaborators is to increase the number of people who
adopt (multiverse-wide) superrational cooperation.

Indeed, it is plausible that small groups of superrationalists should
focus on promoting the idea rather than attempting to help other
superrationalists directly. After all, if one of them can convince only
two others to cooperate superrationally, she already doubles her impact
relative to cooperating on her own. Of course, the two others could also
convince others in turn. Needless to say,
\href{https://en.wikipedia.org/wiki/Logistic_function\#In_ecology:_modeling_population_growth}{\emph{spreading
the idea saturates at some point}}. At least when all humans are
convinced of superrational cooperation, the idea cannot be spread
further. More realistically, we will run out of people who are willing
to think about such seemingly speculative topics.

\hypertarget{artificial-intelligence}{\subsection{Artificial
intelligence}\label{artificial-intelligence}}

One particularly important way of shaping the future is
\href{https://foundational-research.org/why-altruists-should-focus-on-artificial-intelligence/}{\emph{artificial
intelligence}} \parencite{Bostrom2014-pc}. Given our
newfound knowledge, we can differentiate between AI safety measures that
are inspired by superrational cooperation and AI safety measures that
are not.

\subsubsection{AI safety not based on superrationality-related
considerations}\label{ai-safety-not-based-on-superrationality-related-considerations}

The goal of current AI safety research is to make AIs behave in ways
that are more compatible with some human value system.\footnote{For
  example, the \href{https://intelligence.org/research/}{\emph{Machine
  Intelligence Research Insitute's ``Research'' page}} is titled
  ``Aligning advanced AI with human interests''. Another AI safety
  organization even mentions it in their name: the
  \href{http://humancompatible.ai/about}{\emph{Center for
  Human-Compatible AI}}. Also consider the
  \href{https://futureoflife.org/ai-principles/}{\emph{Asilomar AI
  principles}} and the discussion of value loading by
  \parencite{Bostrom2014-pc}.} From a multiverse-wide
cooperation perspective, this is positive to the extent that human
values correlate with the values of other evolved agents in the
multiverse. A human-controlled, non-superrational outcome may
nonetheless be suboptimal from an MSR perspective.

Imagine a distant civilization of billions of happy, law-abiding,
art-producing, yet, from a human perspective, ugly-looking
extraterrestrials. Each year, they enslave or kill trillions of other,
less intelligent extraterrestrials, such that the number of miserable
lives and involuntary deaths caused by the civilization is orders of
magnitude higher than the number of positive lives it supports. Most
people on Earth may not care about this civilization at all because it
contains no humans. Some may only care about the smart extraterrestrials
and thus evaluate the society very positively
\parencite{Kagan2016-gc}. However, I suspect that many of
those who care at all about distant ugly aliens also care about less
intelligent aliens. These people would evaluate the civilization as far
less positive. Similarly, many superrationalists in the multiverse may
not evaluate our civilization positively if it were to continue its
current mistreatment of animals.

Another concern is that a civilization might prioritize near-view values
when value loading an AI. This suggests that even if our values
resembled those of other civilizations, the goals we give to an AI might
differ significantly from what extraterrestrials care about in our
civilization.

\href{https://foundational-research.org}{\emph{FRI}} has previously investigated ways of making AI
alignment failures less harmful by focusing on avoiding very bad AI outcomes rather than
attempting more fine-grained control
\parencite{Gloor2016-oy}. One motivation to do so is this
approach's cooperativeness: different value systems may disagree on what
future should be created. For example,
\href{https://www.hedweb.com/hedab.htm}{\emph{some}} want the universe
to be filled with concentrated pleasure, whereas others envision human
civilizations of varying social, economic and political systems, often
rid of poverty, diseases, involuntary death, and so forth. However,
different value systems often agree on a large set of futures that
should \emph{not} be created. Things like premature death, suffering,
war, and extinction are almost universally seen as bad. Avoiding
dystopian scenarios can thus benefit a wider range of value systems.
Another MSR-based reason to focus on very bad outcomes is that, because
our civilization will be destroyed in all of them, avoiding them evokes
abstract construals. These probably do a better job than concrete
construals at approximating what extraterrestrials care about in our
civilization (cf. section
\ref{values-and-distance}). However, making AI more fail-safe from a MSR perspective
would be less focused on preventing outcomes with a lot of suffering
than FRI's previous work. Also, its level of priority depends on its
feasibility. Whereas heuristic arguments suggest that merely avoiding
bad outcomes might be more feasible than working toward fully
human-aligned AI, it has so far proven difficult to do any concrete work
in the area. Overall, I think it is an approach worth investigating
further in the context of superrational compromise, but not likely to be
a top intervention.

\subsubsection{Multiverse-wide superrational cooperation-inspired
value-loading}\label{multiverse-wide-superrational-cooperation-inspired-value-loading}

In section \ref{the-compromise-problem}, we viewed compromising as a one-time process, in
which all agents adopt a new utility function \(u^{*}\) to maximize in
their part of the multiverse. If they indeed acted as though they now
only care about maximizing \(u^{*}\), the natural consequence would be
to push for AI values that are closer to \(u^{*}\). One way to do this
is to directly implement the value systems that one would also spread to
other humans (discussed in section
\ref{moral-advocacy}). For
example, one could try to make future AIs hold a wider variety of values
(see section \ref{pluralism}) or
perhaps prioritize universal concerns a bit more (see section
\ref{universalist-values}).

More robustly, one could directly implement a pointer to the aggregated
consequentialist far values of superrationalists in the multiverse.
Indeed, extracting \(u^{*}\) from the multiverse appears to be roughly
as difficult to specify as extracting goals of humans. Just as one could
identify humans in the world model, extract their goals and aggregate
them, so one could identify superrational cooperators, extract their
goals and aggregate them.\footnote{Of course, identifying superrational
  cooperators in a world model may be more or less difficult than
  identifying humans in the world model. My tentative guess would be
  that it is easier, because I think the category of superrationalists
  can be described more succinctly than the category of humans, but of
  course I am not very confident in this claim. Similarly, it may be
  that MSR-type aggregation (e.g., variance normalization) is more or
  less difficult to implement than the aggregation procedures one would
  implement for humans.} (A somewhat similar proposal was made by Bostrom
\parencite{Bostrom2014-gy}; see section
\ref{acausal-trade}.)

Of course, it is unlikely that superrationalists could convince the
majority of people of such goal systems. Nonetheless, at this early
stage of the field of AI safety, it seems useful to also explore
unrealistic proposals like this one. Additionally, less attractive goal
functions may still be relevant as backups (see
\href{https://foundational-research.org/files/backup-utility-functions.pdf}{\emph{Oesterheld
2016}}).

Another disadvantage of this approach is that it breaks if the analysis
underlying our specification of \(u^{*}\) is incorrect. For instance, if
MSR does not work at all, then making AI care about ET values directly
is much worse than simply implementing our own values.

\hypertarget{making-an-ai-come-up-with-superrational-cooperation-on-its-own}{\subsubsection{Making
an AI come up with superrational cooperation on its
own}\label{making-an-ai-come-up-with-superrational-cooperation-on-its-own}}

Instead of directly implementing our compromise utility function, we
could also make the AI come up with such a compromise on its own. This
has several advantages. Most importantly, it protects against some
possible mistakes on our side. If, say, we were unable to find the
correct superrational compromise, we could let the AI find it on its
own. Also, the AI may at some point discover that there are no other
agents in the multiverse after all, at which point it could choose to
stop wasting further resources into compromising with these nonexistent
agents.

The primary way of getting an AI to compromise superrationally is to
ensure that it reasons in accordance with the right decision
theory.\footnote{Reasoning in accordance with some decision theory is
  not meant to imply that the decision theory is hard-coded into the AI.
  Instead, the decision theory that an AI uses may be the result of
  particular choices of architecture. To ensure that the AI reasons in
  accordance with the right decision theory, we would then have to find
  out what the decision-theoretical implications of different AI design
  choices are and ensure that these receive due consideration in the
  construction of intelligent machines.} \footnote{There are other ways
  to make it more likely that the AI applies MSR. For example, one could
  ensure that its epistemology enables it to infer the existence of
  other universes that cannot be observed directly. We could also think
  of an AI that would accept MSR, but somehow never has the idea of MSR.
  Much more plausibly, some AIs will simply not care about distant
  universes in a consequentialist way. However, all of these parameters
  seem more difficult to influence than the AI's decision theory.} This
in turn involves advancing the field of decision theory and
investigating possible ways of implementing decision theories in AI
systems. Given how both of these areas seem neglected and gains from
trade may be quite significant, I could very well imagine that
interventions in this area are among the most effective of those
hitherto considered by effective altruists.

\paragraph{Value loading is still
necessary}\label{value-loading-is-still-necessary}

If all acausal collaborators settle on maximizing some utility function,
perhaps value loading is unnecessary for AIs with the right decision
theories anyway? After all, once such an AI joins the MSR compromise, it
will update its utility function accordingly -- regardless of whether it
originally wants to maximize paperclips or to reduce suffering.

But this reasoning seems unsound. While all AIs may settle on the same
compromise utility function, the original value system of the AI still
affects what that compromise utility function ends up being. Without
superrationality, value loading affects the dominant values of one AI.
If there are \(m\) superrationalist civilizations, then each can affect
the dominating values in \(m\) AIs by \(\frac{1}{m}\) (assuming that all
civilizations are equally powerful, etc.). So, proper value loading is
actually just as effective as before, if not more because of gains from
trade. Even if we manage to reliably make the AI join a superrational
compromise, we will still want to make it value the right things.

I am uncertain about whether some version of the above argument against
value loading may work after all. Even if all AIs have
``\href{https://wiki.lesswrong.com/wiki/Paperclip_maximizer}{\emph{paperclipper}}
values'', perhaps they would still recognize that other value systems
originally had all the power, causing the AIs to give them higher
compromise weights? Similarly, one may have some intuitions that value
loading superrational AIs should not be necessary, given that it just
moves power between superrational cooperators. However, at this point,
these are merely intuitions and not arguments. Except from potentially
guiding future research, I do not think they should affect our
priorities.

\hypertarget{compromise-friendly-backup-utility-functions}{\paragraph{Compromise-friendly
backup utility
functions}\label{compromise-friendly-backup-utility-functions}}

Even though value loading is still necessary, we can nonetheless benefit
our superrational collaborators (and thereby ourselves) in cases where
value-loading fails. Even if an AI has values that differ from those of
humans, it may still trade with other civilizations. Hence, we should
attempt to load it with values that especially lend themselves to
compromise, such that the other value systems benefit as much as
possible (cf.
\href{http://www.nickbostrom.com/papers/porosity.pdf}{\emph{Bostrom
2014b}}). Because one would usually attempt to load an AI with one's own
values, such a compromise-friendly (``porous'', in Bostrom's
terminology) utility function would usually only be a backup (see
\href{https://foundational-research.org/files/backup-utility-functions.pdf}{\emph{Oesterheld
2016}}).
